{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-65c670b53b46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     'Pendulum-v0',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menv_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     print(\n",
      "\u001b[0;32m<ipython-input-3-65c670b53b46>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     'Pendulum-v0',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menv_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     print(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "env_names = [\n",
    "#     'Acrobot-v1',\n",
    "    'CartPole-v1',\n",
    "#     'MountainCar-v0',\n",
    "#     'Pendulum-v0',\n",
    "]\n",
    "envs = [gym.make(env_name) for env_name in env_names]\n",
    "for env in envs:\n",
    "    print(\n",
    "        str(env),\n",
    "        env._max_episode_steps,\n",
    "        env.observation_space,\n",
    "        env.action_space,\n",
    "        env.spec.reward_threshold,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_out_length(env):\n",
    "    input_length = env.observation_space.shape[0]\n",
    "    if isinstance(env.action_space, gym.spaces.box.Box):\n",
    "        output_length = env.action_space.shape[0]\n",
    "    elif isinstance(env.action_space, gym.spaces.discrete.Discrete):\n",
    "        output_length = env.action_space.n\n",
    "    return input_length, output_length\n",
    "\n",
    "# class PolicyNN(nn.Module):\n",
    "#     def __init__(self, input_length, output_length, is_distribution=True, output_quantization_levels=11):\n",
    "#         super(PolicyNN, self).__init__()\n",
    "#         self.is_distribution = is_distribution\n",
    "#         if not self.is_distribution:\n",
    "#             output_length *= output_quantization_levels\n",
    "#         self.fc = nn.Linear(input_length, output_length)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.fc(x)\n",
    "#         return F.softmax(x, dim=1)\n",
    "# #         if self.is_distribution:\n",
    "# #             return F.softmax(x, dim=1)\n",
    "# #         else:\n",
    "# #             return x\n",
    "\n",
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self, input_length, output_length):\n",
    "        super(PolicyNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_length, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, output_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "def select_action_from_policy(model, state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    x = model(state)\n",
    "    m = Categorical(x)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)\n",
    "\n",
    "def standard_normal(x):\n",
    "    t = torch.FloatTensor(x)\n",
    "    t = (t - t.mean()) / (t.std() + np.finfo(np.float32).eps)\n",
    "    return t\n",
    "\n",
    "def compute_loss(probs, rewards, states, reward_func, gamma=0.95):\n",
    "#     print('rewards', rewards)\n",
    "#     print('states', states)\n",
    "    print(rewards)\n",
    "    rewards = reward_func(rewards, states)\n",
    "    print(rewards)\n",
    "#     print('after reward_func', rewards)\n",
    "    print(sum(rewards))\n",
    "    scaled_rewards = []\n",
    "    # decay rewards with gamma\n",
    "    tail = 0\n",
    "    for reward in rewards[::-1]: # backward\n",
    "        tail = reward + gamma * tail\n",
    "        scaled_rewards.insert(0, tail) # insert at beginning\n",
    "#     print('after gamma', scaled_rewards)\n",
    "#     scaled_rewards = standard_normal(scaled_rewards)\n",
    "#     print('after scale', scaled_rewards)\n",
    "#     print('sum r %.2f' % sum(scaled_rewards))\n",
    "#     print('avg p %.2f' % (sum(math.exp(p) for p in probs) / len(probs)))\n",
    "    loss = 0\n",
    "    for p, r in zip(probs, scaled_rewards):\n",
    "        loss += p * r\n",
    "    loss *= -1\n",
    "    return loss\n",
    "\n",
    "def is_solved(env, episode_rewards, env_desc):\n",
    "    if len(episode_rewards) < 10:\n",
    "        return False\n",
    "    reward_threshold = env.spec.reward_threshold\n",
    "    if reward_threshold is None:\n",
    "            reward_threshold = env_desc.reward_threshold\n",
    "    if sum(episode_rewards[-10:]) / 10.0 >= reward_threshold: # last 10 avg exceeds...\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def train_solve(env, reward_func, num_episodes=10*1000):\n",
    "    input_length, output_length = in_out_length(env)\n",
    "    model = PolicyNN(input_length, output_length, False)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    episode_rewards = []\n",
    "    loss = 0\n",
    "    for episode in range(num_episodes):\n",
    "        probs = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "        state = env.reset()\n",
    "        for t in range(env._max_episode_steps):\n",
    "            action, prob = select_action_from_policy(model, state)\n",
    "            action = np.array([(action - (math.floor(model.output_quantization_levels/2))) * 0.2])\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if episode > 0 and episode % 200 == 0:\n",
    "                env.render()\n",
    "            probs.append(prob)\n",
    "            rewards.append(reward)\n",
    "            states.append(state.copy())\n",
    "            if done:\n",
    "                break\n",
    "        episode_reward = sum(rewards)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        if is_solved(env, episode_rewards):\n",
    "            print('Solved in %s episodes!' % (episode+1))\n",
    "            return model\n",
    "        loss += compute_loss(probs, rewards, states, reward_func)\n",
    "        if episode > 0 and episode % 10 == 0:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('%d \\t %0.3f \\t %d' % (episode, loss.item(), episode_reward))\n",
    "            loss = 0\n",
    "    print('Failed!')\n",
    "    return model\n",
    "    \n",
    "def mountaincar_reward_func(rewards, states):\n",
    "    states = [(state[0]+0.5, state[1]/0.07) for state in states] # normalize\n",
    "    mx = max(state[0] for state in states)\n",
    "    mv = max(state[1] for state in states)\n",
    "    print('%.2f' % max(state[0] for state in states))\n",
    "    for i in range(len(rewards)):\n",
    "        rewards[i] = 0\n",
    "        if states[i][0] == mx:\n",
    "            rewards[i] += mx\n",
    "        if states[i][1] == mv:\n",
    "            rewards[i] += mv\n",
    "    return rewards\n",
    "\n",
    "def pendelum_reward_func(rewards, states):\n",
    "    rewards = [state[0] for state in states]\n",
    "    head = [0] * 100\n",
    "    tail = rewards[-100:]\n",
    "    rewards = head + tail\n",
    "    return rewards\n",
    "\n",
    "# envs = [\n",
    "# #     ('Acrobot-v1',     lambda rewards, states: rewards),\n",
    "# #     ('CartPole-v1',    lambda rewards, states: rewards),\n",
    "# #    ('MountainCar-v0', mountaincar_reward_func)\n",
    "# #     'MountainCarContinuous-v0',\n",
    "#     ('Pendulum-v0',   pendelum_reward_func),\n",
    "# ]\n",
    "# models = []\n",
    "# for env_name, reward_func in envs:\n",
    "#     env = gym.make(env_name)\n",
    "#     print('Doing %s: %s' % (env_name, (\n",
    "#         str(env),\n",
    "#         env._max_episode_steps,\n",
    "#         env.observation_space,\n",
    "#         env.action_space,\n",
    "#         env.spec.reward_threshold,\n",
    "#     )))\n",
    "#     model = train_solve(env, reward_func)\n",
    "#     models.append((env_name, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing CartPole-v1\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....\n",
      "Solved in 2205 episodes!\n",
      "Doing Acrobot-v1\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..........................\n",
      "Solved in 8027 episodes!\n"
     ]
    }
   ],
   "source": [
    "def mountaincar_reward_func(rewards, states):\n",
    "    rewards = [state[0] for state in states]\n",
    "    head = [0] * 150\n",
    "    tail = rewards[-50:]\n",
    "    rewards = head + tail\n",
    "    return rewards\n",
    "\n",
    "def pendelum_reward_func(rewards, states):\n",
    "    rewards = [state[0] for state in states]\n",
    "    head = [0] * 100\n",
    "    tail = rewards[-100:]\n",
    "    rewards = head + tail\n",
    "    return rewards\n",
    "\n",
    "class PolicyShallowNN(nn.Module):\n",
    "    def __init__(self, input_length, output_length):\n",
    "        super(PolicyShallowNN, self).__init__()\n",
    "        self.fc = nn.Linear(input_length, output_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "class PolicyDeepNN(nn.Module):\n",
    "    def __init__(self, input_length, output_length):\n",
    "        super(PolicyDeepNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_length, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, output_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "fields = [\n",
    "    'name',\n",
    "    'model_class',\n",
    "    # the rest are optional, in case it's needed for the env\n",
    "    'reward_func',\n",
    "    'output_quantization_levels',\n",
    "    'output_range',\n",
    "    'reward_threshold',\n",
    "]\n",
    "EnvDescription = collections.namedtuple('EnvDescription', fields, defaults=[None] * len(fields))    \n",
    "\n",
    "env_descriptions = [\n",
    "    EnvDescription(name='CartPole-v1',\n",
    "                   model_class=PolicyShallowNN,\n",
    "                  ),\n",
    "    EnvDescription(name='Acrobot-v1',\n",
    "                   model_class=PolicyShallowNN,\n",
    "                  ),\n",
    "#     EnvDescription(name='Pendulum-v0',\n",
    "#                    model_class=PolicyDeepNN,\n",
    "#                    reward_func=pendelum_reward_func,\n",
    "#                    output_quantization_levels=101,\n",
    "#                    output_range=[-2, 2],\n",
    "#                    reward_threshold=-150,\n",
    "#                   ),\n",
    "#     EnvDescription(name='MountainCar-v0',\n",
    "#                    model_class=PolicyDeepNN,\n",
    "#                    reward_func=mountaincar_reward_func\n",
    "#                   ),\n",
    "]\n",
    "\n",
    "def reward_threshold(env, env_desc):\n",
    "    if env_desc.reward_threshold is not None:\n",
    "        return env_desc.reward_threshold\n",
    "    else:\n",
    "        return env.spec.reward_threshold\n",
    "\n",
    "def is_solved(env, episode_rewards, reward_threshold):\n",
    "    if len(episode_rewards) < 10:\n",
    "        return False\n",
    "    if sum(episode_rewards[-10:]) / 10.0 >= reward_threshold: # last 10 avg exceeds...\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def compute_loss(probs, rewards, states=None, reward_func=None, gamma=0.99):\n",
    "#     print('rewards', rewards)\n",
    "#     print('states', states)\n",
    "    if reward_func is not None:\n",
    "        rewards = reward_func(rewards, states)\n",
    "#     print('after reward_func', rewards)\n",
    "#     print(sum(rewards))\n",
    "    scaled_rewards = []\n",
    "    # decay rewards with gamma\n",
    "    tail = 0\n",
    "    for reward in rewards[::-1]: # backward\n",
    "        tail = reward + gamma * tail\n",
    "        scaled_rewards.insert(0, tail) # insert at beginning\n",
    "#     print('after gamma', scaled_rewards)\n",
    "#     scaled_rewards = standard_normal(scaled_rewards)\n",
    "#     print('after scale', scaled_rewards)\n",
    "#     print('sum r %.2f' % sum(scaled_rewards))\n",
    "#     print('avg p %.2f' % (sum(math.exp(p) for p in probs) / len(probs)))\n",
    "    loss = 0\n",
    "    for p, r in zip(probs, scaled_rewards):\n",
    "        loss += p * r\n",
    "    loss *= -1\n",
    "    return loss\n",
    "\n",
    "def train_solve(env_desc, num_episodes=100*1000):\n",
    "    print('Doing %s' % env_desc.name)\n",
    "    env = gym.make(env_desc.name)\n",
    "    input_length, output_length = in_out_length(env)\n",
    "    if env_desc.output_quantization_levels is not None:\n",
    "        output_length *= env_desc.output_quantization_levels\n",
    "    model = env_desc.model_class(input_length, output_length) #PolicyNN(input_length, output_length)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    episode_rewards = []\n",
    "    loss = 0\n",
    "    for episode in range(num_episodes):\n",
    "        if episode > 0: print('.', end=('' if bool(episode % 100) else '\\n'))\n",
    "        probs, rewards, states = [], [], []\n",
    "        state = env.reset()\n",
    "        for t in range(env._max_episode_steps):\n",
    "            action, prob = select_action_from_policy(model, state)\n",
    "            if env_desc.output_quantization_levels is not None:\n",
    "                action = env_desc.output_range[0] + action * float(env_desc.output_range[1] - env_desc.output_range[0]) / (env_desc.output_quantization_levels-1)\n",
    "                action = np.array([action])\n",
    "            state, reward, done, _ = env.step(action)\n",
    "#             if episode > 0 and episode % 200 == 0:\n",
    "#                 env.render()\n",
    "            probs.append(prob)\n",
    "            rewards.append(reward)\n",
    "            states.append(state.copy())\n",
    "            if done:\n",
    "                break\n",
    "        episode_reward = sum(rewards)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        if is_solved(env, episode_rewards, reward_threshold(env, env_desc)):\n",
    "            print('\\nSolved in %s episodes!' % (episode+1))\n",
    "            return env, model\n",
    "        loss += compute_loss(probs, rewards, states, env_desc.reward_func)\n",
    "        if episode > 0 and episode % 10 == 0:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             print('%d \\t %0.3f \\t %d' % (episode, loss.item(), episode_reward))\n",
    "            loss = 0\n",
    "    print('\\nFailed!')\n",
    "    return env, model\n",
    "\n",
    "envs, models = [], []\n",
    "for env_desc in env_descriptions:\n",
    "    env, model = train_solve(env_desc)\n",
    "    envs.append(env)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EnvDescription(name='CartPole-v1', model_class=<class '__main__.PolicyShallowNN'>, reward_func=None, output_quantization_levels=None, output_range=None, reward_threshold=None), EnvDescription(name='Acrobot-v1', model_class=<class '__main__.PolicyShallowNN'>, reward_func=None, output_quantization_levels=None, output_range=None, reward_threshold=None)] \n",
      "\n",
      " [<TimeLimit<CartPoleEnv<CartPole-v1>>>, <TimeLimit<AcrobotEnv<Acrobot-v1>>>] \n",
      "\n",
      " [PolicyShallowNN(\n",
      "  (fc): Linear(in_features=4, out_features=2, bias=True)\n",
      "), PolicyShallowNN(\n",
      "  (fc): Linear(in_features=6, out_features=3, bias=True)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "def select_action_from_policy_best(model, state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = model(state)\n",
    "    if probs[0][0] > probs[0][1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def play(env_desc, env, model):\n",
    "    state = env.reset()\n",
    "    for t in range(1, env._max_episode_steps):\n",
    "        action, _ = select_action_from_policy(model, state)\n",
    "        state, _, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "which = 1\n",
    "print(env_descriptions, '\\n\\n', envs, '\\n\\n', models)\n",
    "play(env_descriptions[which], envs[which], models[which])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
