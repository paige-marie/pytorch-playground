{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state is represented by 4 numbers:\n",
    "\n",
    "The cart position x from -2.4 to 2.4.\n",
    "\n",
    "The cart velocity v\n",
    "\n",
    "The pole angle θ with respect to the vertical from -12 to 12 degrees (from -0.21 to 0.21 in radians)\n",
    "\n",
    "The pole angular velocity ω. This is the rate of change of θ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from itertools import count\n",
    "from random import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from pendulum import PendulumEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "env = PendulumEnv(render_mode='human')\n",
    "#print(env._max_episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "observation state is  [1. 0. 0.]\n",
      "0.04022\n"
     ]
    }
   ],
   "source": [
    "#floats between -2.0 and 2.0\n",
    "import random\n",
    "def select_action_random(state):\n",
    "    rand = random.uniform(-2.0, 2.0)\n",
    "    return [rand]\n",
    "    \n",
    "\n",
    "def goodness_score(select_action, num_episodes=10):\n",
    "    \n",
    "    num_steps = 500\n",
    "    ts = []\n",
    "    #for episode in range(1):\n",
    "    for episode in range(num_episodes):\n",
    "#         state = env.reset(options={\"x_init\":np.pi, \"y_init\":1.0})[0]\n",
    "        state = env.reset()[0]\n",
    "        print(\"observation state is \", state)\n",
    "        for t in range(1, num_steps+1):\n",
    "            #print(state)\n",
    "            action = select_action(state)\n",
    "            #print(\"action is \", action)\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "            #print(\"state is \", state)\n",
    "            if done:\n",
    "                #print(\"step is \", t)\n",
    "                break\n",
    "        ts.append(t)\n",
    "    score = sum(ts) / (len(ts)*num_steps)\n",
    "    return score\n",
    "\n",
    "print(goodness_score(select_action_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNN, self).__init__()\n",
    "        self.fc = nn.Linear(3,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def select_action_from_policy(model, state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    y = model(state)\n",
    "    #y = y.detach().numpy()\n",
    "    return y\n",
    "\n",
    "def select_action_from_policy_best(model, state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = model(state)\n",
    "    if probs[0][0] > probs[0][1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_untrained = PolicyNN()\n",
    "\n",
    "# print(\n",
    "#     goodness_score(lambda state: select_action_from_policy(model_untrained, state[0])),\n",
    "#     goodness_score(lambda state: select_action_from_policy_best(model_untrained, state))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 21 -41.483856201171875\n",
      "1 21 -33.21754837036133\n",
      "2 22 -40.19142150878906\n",
      "3 23 -46.6368522644043\n",
      "4 24 -51.639686584472656\n",
      "5 25 -54.00933074951172\n",
      "6 26 -52.32860565185547\n",
      "7 27 -45.175716400146484\n",
      "8 30 -67.6634521484375\n",
      "9 33 -63.42487335205078\n"
     ]
    }
   ],
   "source": [
    "model = PolicyNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_wont_work(num_episodes=100):\n",
    "    num_steps = 500\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        for t in range(1, num_steps+1):\n",
    "            action = select_action_from_policy(model, state)\n",
    "            print(action)\n",
    "            state, _, done, _, info = env.step(action[0])\n",
    "            if done:\n",
    "                break\n",
    "        loss = 1.0 - t / num_steps\n",
    "        # this doesn't actually work, because\n",
    "        # the loss function is not an explicit\n",
    "        # function of the model's output; it's\n",
    "        # a function of book keeping variables\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # AttributeError: 'float' object has no attribute 'backward'\n",
    "        optimizer.step()\n",
    "\n",
    "def train_simple(num_episodes=10):\n",
    "    num_steps = 10000\n",
    "    ts = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        probs = []\n",
    "        for t in range(1, num_steps+1):\n",
    "            action = select_action_from_policy(model, state)\n",
    "            probs.append(action[0])\n",
    "            #state, _, done, _ = env.step(action)\n",
    "            state, reward, done, truncated, info = env.step(action[0].detach().numpy())\n",
    "            if done:\n",
    "                break\n",
    "        loss = 0\n",
    "        for i, prob in enumerate(probs):\n",
    "#             loss += -1 * (t - i) * prob\n",
    "             loss += -1 * torch.abs(prob - reward)\n",
    "        print(episode, t, loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ts.append(t)\n",
    "        if len(ts) > 10 and sum(ts[-10:])/10.0 >= num_steps * 0.95:\n",
    "            print('Stopping training, looks good...')\n",
    "            return\n",
    "\n",
    "train_simple()\n",
    "#train_wont_work()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    goodness_score(lambda state: select_action_from_policy(model, state)[0]),\n",
    "    goodness_score(lambda state: select_action_from_policy_best(model, state))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
